{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.362084150314331\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Wed Dec 29 15:53:19 2019\n",
    "\n",
    "@author: zionzhou\n",
    "\n",
    "Introduction:\n",
    "    Hi, the file introduce a function of calculating the sentiment of articles. The result will be export as\n",
    "    a csv file, containing columns (article_id, pos_word, neg_word, total_word, sentiment).\n",
    "    \n",
    "    The function contains the method of tokenization, contraction expansion, lemmatization and stop-words exclusion.\n",
    "    In my analysis, by default, I don't use the contraction expansion and lemmatization methods, because the two\n",
    "    have little contribution on the sentiment analysis from my analysis. However, I include these method in the \n",
    "    function once anyone want to use them in his/her project. \n",
    "    \n",
    "    It's worth noting that the counting method of total words I used is to count the words after excluding the stop \n",
    "    words from the article. Most of the stop words are useless and influential to our result. As our sentiment\n",
    "    analysis method is calculated by the difference of positive words and negative words divided by the total words.\n",
    "    The words that counted in total words should have the sentiment potential instead of the meaningless stop words,\n",
    "    so we would exclude the stop words before counting the total words.\n",
    "    \n",
    "\"\"\"\n",
    "# import the packages needed first\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from contractions import CONTRACTION_MAP\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Record the initial time\n",
    "time1 = time.time()\n",
    "# Read the article file and the postive, negative list\n",
    "# Convert the data in dictionary format would boost the speed when analysing afterwards\n",
    "df = pd.read_csv('articles.csv')\n",
    "df = df.to_dict()\n",
    "negative_list_pre = tuple(pd.read_excel('LoughranMcDonald_SentimentWordLists_2018.xlsx',\n",
    "                                    sheet_name = 'Negative',header = None)[0])\n",
    "positive_list_pre = tuple(pd.read_excel('LoughranMcDonald_SentimentWordLists_2018.xlsx',\n",
    "                                    sheet_name = 'Positive',header = None)[0])\n",
    "negative_list = {char:1 for char in negative_list_pre}\n",
    "positive_list = {char:1 for char in positive_list_pre}\n",
    "# Define the stop_words\n",
    "StopWords = stopwords.words('english')\n",
    "# As the stop_words we used is not customized for our analysis, some words in negative/positive list are also in the stop words list\n",
    "# Therefore we choose to redefine a new cleaned-stop-words that exclude words in either negative list or positive list\n",
    "def cleaned_stopwords(pos_list,neg_list,stopwords):\n",
    "    c_list=[words for words in stopwords if words.upper() not in (negative_list_pre+positive_list_pre)]\n",
    "    return c_list\n",
    "\n",
    "CleanedStopWords = cleaned_stopwords(positive_list_pre,negative_list_pre,StopWords)\n",
    "\n",
    "# Define the function of calculating the sentiment score of a collection of articles\n",
    "# Here, by default, we don't use the Expand_Contraction method and Lemmatization method\n",
    "# If you want to use any of these two methods, set the corresponding status value equal to 1\n",
    "\n",
    "def calculate_sentiment(DataFrame,negative_list,positive_list,expand_contraction=0,Lemmatization=0):\n",
    "    try:\n",
    "        # At first, define a empty dictionary to store the outcome we need\n",
    "        result = {'article_id':list(df['aid'].values()), 'pos_word':[], 'neg_word':[],'total_word':[], 'sentiment':[]}\n",
    "        # define the function of calculating the sentiment score and negative/positive words\n",
    "        def sentiment_score(words,pos_list,neg_list):\n",
    "            pos,neg = 0,0\n",
    "            tot_wds = len(words)\n",
    "            for word in words:\n",
    "                try:\n",
    "                    positive_list[word] == 1\n",
    "                    pos += 1\n",
    "                except:\n",
    "                    try:\n",
    "                        negative_list[word] == 1\n",
    "                        neg += 1\n",
    "                    except:\n",
    "                        continue \n",
    "            senti = (pos-neg)/tot_wds\n",
    "            return pos,neg,tot_wds,senti\n",
    "        # Define the function of expand contractions\n",
    "        def expand_contractions(text,contraction_mapping=CONTRACTION_MAP):\n",
    "            contractions_pattern=re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)\n",
    "            def expand_match(contraction):\n",
    "                match=contraction.group(0)\n",
    "                first_char=match[0]\n",
    "                expanded_contraction=contraction_mapping.get(match)\\\n",
    "                    if contraction_mapping.get(match)\\\n",
    "                    else contraction_mapping.get(match.lower())\n",
    "                expanded_contraction=first_char+expanded_contraction[1:]\n",
    "                return expanded_contraction\n",
    "            expanded_text=contractions_pattern.sub(expand_match,text)\n",
    "            expanded_text=re.sub(\"'\",\"\", expanded_text)\n",
    "            return expanded_text\n",
    "        \n",
    "        # Define the function of tokenizing the article into words and cleaning the tokens\n",
    "        def cleaned_tokenize(doc, stop_words = ()):\n",
    "            doc=BeautifulSoup(doc, 'html.parser').get_text()\n",
    "            if expand_contraction == 1:\n",
    "                doc=expand_contractions(doc)\n",
    "            doc=re.sub(r'[^a-zA-Z\\s]',' ',doc)\n",
    "            doc=doc.lower()\n",
    "            tokens = word_tokenize(doc)\n",
    "            if Lemmatization == 0:\n",
    "                cleaned_tokens = [token.upper() for token in tokens if (token not in stop_words and len(token)>1)]\n",
    "            else:\n",
    "                cleaned_tokens = []\n",
    "                for token, tag in pos_tag(tokens):\n",
    "                    if tag.startswith(\"NN\"):\n",
    "                        pos = 'n'\n",
    "                    elif tag.startswith('VB'):\n",
    "                        pos = 'v'\n",
    "                    else:\n",
    "                        pos = 'a'\n",
    "                    lemmatizer = WordNetLemmatizer()\n",
    "                    token = lemmatizer.lemmatize(token, pos)\n",
    "                    if len(token) > 0 and token not in string.punctuation and token not in stop_words:\n",
    "                        cleaned_tokens.append(token.upper())\n",
    "            return cleaned_tokens\n",
    "        # Calculate the sentiment score and other necessary items and store them in pre-created dictionary\n",
    "        for i in range(len(DataFrame['aid'])):\n",
    "            content = DataFrame['content'][i]\n",
    "            tokens = cleaned_tokenize(content, stop_words = CleanedStopWords)\n",
    "            pos,neg,tot_words,senti = sentiment_score(tokens,positive_list,negative_list)\n",
    "            result['pos_word'].append(pos)\n",
    "            result['neg_word'].append(neg)\n",
    "            result['total_word'].append(tot_words)\n",
    "            result['sentiment'].append(senti)\n",
    "    # When exception happens, report at the bottom \n",
    "    except Exception as e:\n",
    "        print(\"Exception Occured:\",e)\n",
    "\n",
    "    result=pd.DataFrame(data=result)    # transfer the dictionary into dataframe\n",
    "    return result\n",
    "# Execute the whole sentiment analysis function\n",
    "# If you want to use the Expand_Contraction or Lemmatization method, add the statement in the variable bracket\n",
    "result = calculate_sentiment(df,negative_list,positive_list)\n",
    "# Output the csv file\n",
    "result.to_csv('result.csv',index=False)\n",
    "# Record the last time\n",
    "time2 = time.time()\n",
    "# Print the time used in total\n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
